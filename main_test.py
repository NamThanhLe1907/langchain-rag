import unicodedata
import uuid
import sys

from dotenv import load_dotenv
load_dotenv()  # T·∫£i bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
from langchain_core.messages import AIMessage, HumanMessage
# Import graph builder ƒë·ªÉ c√≥ part_1_graph
from graph_builder import part_1_graph
from data.state import State
from data.database import update_dates, db
from tools.tool_util import _print_event

class AIAgentFAQ:
    def __init__(self, model_name_llm="gpt-4o-mini", llm_temperature=0.7, k=2):
        # self.faq_file_path = faq_file_path
        self.documents = []
        self.embeddings = None
        self.vectorstore = None
        self.retriever = None
        self.llm = None
        self.chain = None
        self.memory = None
        self.k = k
        self.model_name_llm = model_name_llm
        self.llm_temperature = llm_temperature
        
    #     self.initialize_agent()

    # def initialize_agent(self):
    #     self.load_faq_data()
    #     self.build_vectorstore()
    #     self.build_conversational_chain()

    # def load_faq_data(self):
    #     with open(self.faq_file_path, 'r', encoding='utf-8', errors='replace') as f:
    #         faq_data = json.load(f)
    #     for entry in faq_data:
    #         content = self.clean_text(f"Question: {entry['question']}\\nAnswer: {entry['answer']}")
    #         self.documents.append(Document(page_content=content))
    #     print(f"ƒê√£ t·∫£i {len(self.documents)} t√†i li·ªáu.")

    # def build_vectorstore(self):
    #     self.embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2",
    #                                             model_kwargs={'device': 'cpu'},
    #                                             encode_kwargs={'normalize_embeddings': False})
    #     self.vectorstore = FAISS.from_documents(self.documents, self.embeddings)
    #     self.retriever = self.vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": self.k})

    # def build_conversational_chain(self):
    #     self.llm = ChatOpenAI(
    #         model_name=self.model_name_llm,
    #         temperature=self.llm_temperature,
    #     )
    #     # self.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    #     # self.chain = ConversationalRetrievalChain.from_llm(
    #     #     llm=self.llm,
    #     #     retriever=self.retriever,
    #     #     memory=self.memory,
    #     #     chain_type="stuff"
    #     # )

    # def answer_query(self, query):
    #     clean_query = self.clean_text(query)
    #     result = self.chain.invoke({"question": clean_query})
    #     return result["answer"]

    # def start_conversation(self):
    #     print("Ch√†o m·ª´ng b·∫°n. G√µ 'exit' ƒë·ªÉ tho√°t.")
    #     while True:
    #         query = input("B·∫°n: ").strip().lower()
    #         if query in ["exit", "quit"]:
    #             print("T·∫°m bi·ªát!")
    #             break
    #         # S·ª≠ d·ª•ng chain ƒë∆°n gi·∫£n cho FAQ\n
    #         answer = self.answer_query(query)
    #         print("Agent (FAQ):", answer)

    def clean_text(self, text):
        if not isinstance(text, str):
            text = str(text)
        # S·ª≠ d·ª•ng NFKC ƒë·ªÉ chu·∫©n h√≥a v√† encode v·ªõi 'replace' ƒë·ªÉ x·ª≠ l√Ω c√°c k√Ω t·ª± kh√¥ng h·ª£p l·ªá
        normalized = unicodedata.normalize('NFKC', text)
        return normalized.encode('utf-8', 'replace').decode('utf-8')


    def start_graph_conversation(self):
        """Ch·∫°y h·ªôi tho·∫°i v·ªõi graph agent."""
        
        # C·∫≠p nh·∫≠t DB theo file backup
        update_dates(db)
        
        # T·∫°o ID thread duy nh·∫•t
        thread_id = str(uuid.uuid4())

        # Y√™u c·∫ßu nh·∫≠p Passenger ID ban ƒë·∫ßu
        passenger_id = ""   #Available ID: "3442 587242"
        while not passenger_id:
            try:
                raw_input = input("Nh·∫≠p Passenger ID c·ªßa b·∫°n: ").strip()
                passenger_id = self.clean_text(raw_input)
                if not passenger_id:
                    print("‚ùå Passenger ID kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng. Vui l√≤ng nh·∫≠p l·∫°i!")
            except EOFError:
                print("\n‚õî ƒê√£ nh·∫≠n t√≠n hi·ªáu EOF. Tho√°t ch∆∞∆°ng tr√¨nh.")
                sys.exit(0)

        config = {"configurable": {"thread_id": thread_id, "passenger_id": passenger_id}}
        
        # Kh·ªüi t·∫°o state ban ƒë·∫ßu
        initial_state = {"messages": []}
        print("üé§ B·∫Øt ƒë·∫ßu h·ªôi tho·∫°i theo graph (ch·∫ø ƒë·ªô stream). G√µ 'exit' ƒë·ªÉ tho√°t.")

        while True:
            try:
                raw_query = input("B·∫°n: ").strip()
            except EOFError:
                print("\n‚õî ƒê√£ nh·∫≠n t√≠n hi·ªáu EOF. Tho√°t ch∆∞∆°ng tr√¨nh.")
                sys.exit(0)

            query = self.clean_text(raw_query)  # L√†m s·∫°ch input

            # Ki·ªÉm tra n·∫øu ng∆∞·ªùi d√πng mu·ªën tho√°t
            if query.lower() in ["exit", "quit"]:
                print("üëã T·∫°m bi·ªát! H·∫πn g·∫∑p l·∫°i.")
                sys.exit(0)

            # N·∫øu ng∆∞·ªùi d√πng mu·ªën c·∫≠p nh·∫≠t Passenger ID
            if query.lower().startswith("update passenger id:"):
                new_id = self.clean_text(query.split(":", 1)[1].strip())
                if new_id:
                    config["configurable"]["passenger_id"] = new_id
                    print(f"‚úÖ Passenger ID ƒë√£ c·∫≠p nh·∫≠t th√†nh: {new_id}")
                else:
                    print("‚ùå Kh√¥ng nh·∫≠n ƒë∆∞·ª£c Passenger ID m·ªõi. Vui l√≤ng th·ª≠ l·∫°i.")
                continue  # B·ªè qua x·ª≠ l√Ω query hi·ªán t·∫°i

            # Th√™m message c·ªßa ng∆∞·ªùi d√πng v√†o state
            initial_state["messages"].append(HumanMessage(content=query))

            # Debug: Ki·ªÉm tra messages trong state
            print(f"üì© DEBUG - State hi·ªán t·∫°i: {initial_state['messages']}")

            # Stream events ƒë·ªÉ debug
            _printed = set()
            try:
                events = part_1_graph.stream(initial_state, config=config, stream_mode="values")
                for event in events:
                    _print_event(event, _printed)
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi stream event: {e}")

            # G·ªçi graph v·ªõi state ƒë·ªÉ l·∫•y state m·ªõi
            try:
                new_state = part_1_graph.invoke(initial_state, config=config)
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi invoke graph: {e}")
                break

            # In ph·∫£n h·ªìi t·ª´ assistant
            if new_state.get("messages"):
                last_msg = new_state["messages"][-1]
                print(f"ü§ñ Agent (Graph): {last_msg.content if isinstance(last_msg, AIMessage) else last_msg}")
            else:
                print("ü§ñ Agent (Graph): Kh√¥ng c√≥ ph·∫£n h·ªìi.")

            # C·∫≠p nh·∫≠t state m·ªõi
            initial_state = new_state



        
if __name__ == "__main__":
    agent = AIAgentFAQ()
    agent.start_graph_conversation()
